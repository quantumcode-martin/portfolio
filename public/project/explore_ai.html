<!DOCTYPE html>
<html>
<h2>Introdution</h2>
<p>
    Explore AI started as a school project. During the course, Kohonen maps were introduced to us.
    As I was curious about and wanted to learn more I asked to work on this subject, the teacher gave me an example that I could explore about images color space compression.
    This is how Explore AI started.
</p>
<p>
    As I was happy with the visualizations and found them intuitive, I decided to work on some educative Youtube videos.
    I created the <a href="https://youtube.com/playlist?list=PLQOnbVybQMgNV2GfxSuAXXXYpFLKMqCNb">Explore AI series</a>, teaching artificial intelligence principles using visualizations made with Godot Engine.
    The series covers neural networks, backpropagation (as a supervised learning example) and unsupervised learning with Kmeans and Kohonen maps.
</p>

<h2>Neural Networks</h2>
<p>
Starting from the perceptron:
</p>

<img src="https://quantumcode.company/images/explore_ai/perceptron.png" alt="A perceptron">

<P>I explain step by step how a neural network is formed and how it can achieve to solve non-linear problems. </P>
<img src="https://quantumcode.company/images/explore_ai/neural_network.png" alt="A neural network">


<h2>Backpropagation</h2>
<p>
    I explain the principle of backpropagation. I detail formulas, on a perceptron and then on a full neural network.
    I then give examples and use cases both in classification and regression.
</p>

<p>Here is an example of regressions to approximate some logical operators:</p>
<img src="https://quantumcode.company/images/explore_ai/logical_operators_regression.png" alt="Logical operators regression">

<h2>Unsupervised Learning</h2>
<p>
    Unsupervised learning uses machine learning to analyze and cluster unlabeled datasets. The goal is to find hidden patterns or groupings.
</p>

<h3>Image color reduction problem</h3>

<p>
    Let's consider a 50×50 pixels image. Each pixel's color is encoded on 3bytes, this represents 256×256×256 (=more than 16 millions) different possible colors.
    Of course our image (2,500 pixels) does not need all those colors, so we can try to reduce the amount of colors used.
    A naïve approach would be to select colors spread around the visible spectrum, this can work if we keep a high number of colors.
    But as we select fewer colors, a lot of our selected colors will not be used in the image and we will end up with an image that does not make science anymore.
    Another approach would be to take random colors among the ones in our image.
    This would surely result in better results but would still produce bad images as some specific colors would often not be randomly selected.
</p>

<p>
    A great solution to solve this problem is of course to use unsupervised learning.
    We consider the colors in our image as an unlabeled dataset.
    Using unsupervised machine learning we will try to create clusters.
    Those clusters will represent the most "important" colors in the image. 
</p>

<h3>KMeans algorithm</h3>

<p>Using the KMeans algorithm (detailed in the video) we can achieve some impressive results. Here is an example with a 50×50 pixels image represented using only 25 colors.</p>
<img src="https://quantumcode.company/images/explore_ai/kmeans.png" alt="Kmean algorithm">

<p>But because we use random intializations, it sometimes happens to get less impressive results.</p>
<img src="https://quantumcode.company/images/explore_ai/kmeans_bad.png" alt="Kmean algorithm, bad result">
<p>As you can see we still recognize the image, but the colors are not great.</p>

<p>Note that some Kmeans initializations techniques exists but I decided not to explore this part, and create my own initialization as detailed below.</p>

<h3>Kohonen maps</h3>

<p>
    Another unsupervised learning technique is called Kohonen maps, or self-organizing maps (detailed in the video).
    I really enjoy working with this technique because I think it is an intuitive algorithm, and the visualizations are awesome.
    The principle is to create a map with linked nodes, then for each data in the dataset, attract the closest node towards the data and attract it's neighbors too.
    <br> The results are great and it is not prone to randomness as the initialization is done with a map spread around the dataset space. <br>
    I also worked on 3D Kohonen maps to better cover the color space (I first used 2D to make the examples easier).
</p>

<img src="https://quantumcode.company/images/explore_ai/kohonen_map.png" alt="Kohonen map">

<img src="https://quantumcode.company/images/explore_ai/3d_kohonen.png" alt="Kohonen map in 3D">

<p>
    A drawback of Kohonen maps in this case is that they dedicate nodes to highlight distance between clusters.
    So if our image has two very distinct colors, it is possible that the algorithm assign some colors in-between to indicate that those colors are very different.
    We can see this in the previous image, the encircled colors are used by less than 2 pixels, and they are here to show that the purple color is "far" from the other ones. <br>
    This behavior is very useful in many use cases as the resulting Kohonen map has not only information about clusters but also about clusters relationships. <br>
    But in this case we do not really need this and we would prefer each node to be used by many pixels. <br>
    This is how I got the idea to initiate a KMeans algorithm using the result of a Kohonen map. 
</p>

<h3>Kohonen map × KMeans</h3>
<p>
    The results are very promising, the Kohonen×KMean algorithm is beating both Kmeans and Kohonen maps in terms of error.
    This shows that the Kmeans efficiently solve the unused node issue produced by the Kohonen map.
</p>
<img src="https://quantumcode.company/images/explore_ai/kohonen_kmeans.png" alt="Kohonen map in 3D">


</html>